{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import time\n",
    "import glob\n",
    "import re\n",
    "import pandas as pd\n",
    "from functools import reduce\n",
    "from klepto.archives import dir_archive\n",
    "\n",
    "import numpy as np\n",
    "from tqdm.auto import tqdm\n",
    "import coffea.processor as processor\n",
    "from coffea.processor.accumulator import AccumulatorABC\n",
    "from coffea.analysis_objects import JaggedCandidateArray\n",
    "from coffea.btag_tools import BTagScaleFactor\n",
    "from coffea import hist\n",
    "import pandas as pd\n",
    "import uproot_methods\n",
    "import uproot\n",
    "import awkward\n",
    "import copy\n",
    "\n",
    "%matplotlib inline\n",
    "import matplotlib\n",
    "import matplotlib.pyplot as plt\n",
    "from matplotlib.colors import LogNorm\n",
    "\n",
    "from Tools.config_helpers import *\n",
    "from Tools.helpers import mergeArray, mt, get_scheduler_address\n",
    "\n",
    "from Tools.objects import Collections\n",
    "from Tools.cutflow import Cutflow\n",
    "\n",
    "from Tools.WH_objects import *\n",
    "\n",
    "# This just tells matplotlib not to open any\n",
    "# interactive windows.\n",
    "matplotlib.use('Agg')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "def pad_and_flatten(val): \n",
    "    try:\n",
    "        return val.pad(1, clip=True).fillna(0.).flatten()#.reshape(-1, 1)\n",
    "    except AttributeError:\n",
    "        return val.flatten()\n",
    "\n",
    "#os.environ['KERAS_BACKEND'] = 'theano'\n",
    "#from keras.models import load_model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "10000\n"
     ]
    }
   ],
   "source": [
    "import sys\n",
    "sys.setrecursionlimit(10000)\n",
    "print(sys.getrecursionlimit())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "#global variables... to avoid making bugs!!!\n",
    "processesList = ['mC750_l1', 'WJets', 'TTJets', 'ST', 'ttW', 'WW', 'ZNuNu', 'QCD', 'Data']\n",
    "#processesList = ['mC750_l1', 'WJetsOld', 'WJetsNew']\n",
    "#processesList = ['mC750_l1', 'ttbarOld', 'ttbarNew']\n",
    "linesList= ['filters', 'triggers','skim', 'Exactly 1 e or mu',  'MET>250', 'N_fatjet>1', 'min_dphiFatJetMet4', 'dphiDiFatJet', 'minmth>200', 'njet veto', 'N_wtag>0', 'N_htag>0', 'N_wtag>0, N_wtag>0']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "class analysisProcessor(processor.ProcessorABC):\n",
    "    \"\"\"Processor used for running the analysis\"\"\"\n",
    "    def __init__(self):\n",
    "        \n",
    "        ## load b-tag SFs\n",
    "        #self.btag_sf = BTagScaleFactor(os.path.expandvars(\"$TWHOME/data/DeepCSV_102XSF_V1.btag.csv.gz\", \"reshape\")\n",
    "\n",
    "        ## load the NN\n",
    "        #self.model = load_model('../ML/data/training.h5')\n",
    "        #self.stds  = pd.read_json('../ML/data/stds.json').squeeze()\n",
    "        #self.means = pd.read_json('../ML/data/means.json').squeeze()\n",
    "        \n",
    "        # we can use a large number of bins and rebin later\n",
    "        dataset_axis        = hist.Cat(\"dataset\",   \"Primary dataset\")\n",
    "        pt_axis             = hist.Bin(\"pt\",        r\"$p_{T}$ (GeV)\", 1000, 0, 1000)\n",
    "        p_axis              = hist.Bin(\"p\",         r\"$p$ (GeV)\", 1000, 0, 2500)\n",
    "        ht_axis             = hist.Bin(\"ht\",        r\"$H_{T}$ (GeV)\", 500, 0, 5000)\n",
    "        mass_axis           = hist.Bin(\"mass\",      r\"M (GeV)\", 1000, 0, 2000)\n",
    "        eta_axis            = hist.Bin(\"eta\",       r\"$\\eta$\", 60, -5.5, 5.5)\n",
    "        delta_axis          = hist.Bin(\"delta\",     r\"$\\delta$\", 100,0,10 )\n",
    "        multiplicity_axis   = hist.Bin(\"multiplicity\",         r\"N\", 20, -0.5, 19.5)\n",
    "        norm_axis           = hist.Bin(\"norm\",         r\"N\", 25, 0, 1)\n",
    "\n",
    "        self._accumulator = processor.dict_accumulator({\n",
    "            #\"MET_pt_baseline\" :          hist.Hist(\"Counts\", dataset_axis, pt_axis),\n",
    "            #\"HT_baseline\" :              hist.Hist(\"Counts\", dataset_axis, ht_axis),\n",
    "            #\"mtb_min_baseline\" :         hist.Hist(\"Counts\", dataset_axis, mass_axis),\n",
    "            #\"MET_pt\" :          hist.Hist(\"Counts\", dataset_axis, pt_axis),\n",
    "            #\"HT\" :              hist.Hist(\"Counts\", dataset_axis, ht_axis),\n",
    "            #\"mtb_min\" :         hist.Hist(\"Counts\", dataset_axis, mass_axis),\n",
    "            #\"MET_pt_CR\" :       hist.Hist(\"Counts\", dataset_axis, pt_axis),\n",
    "            #\"HT_CR\" :           hist.Hist(\"Counts\", dataset_axis, ht_axis),\n",
    "            #\"mtb_min_CR\" :      hist.Hist(\"Counts\", dataset_axis, mass_axis),\n",
    "            #\"lead_AK8_pt\" :     hist.Hist(\"Counts\", dataset_axis, pt_axis),\n",
    "            #\"W_pt\" :            hist.Hist(\"Counts\", dataset_axis, pt_axis),\n",
    "            #\"H_pt\" :            hist.Hist(\"Counts\", dataset_axis, pt_axis),\n",
    "            #\"W_eta\" :           hist.Hist(\"Counts\", dataset_axis, eta_axis),\n",
    "            #\"H_eta\" :           hist.Hist(\"Counts\", dataset_axis, eta_axis),\n",
    "            \n",
    "            \"met_CR\":           hist.Hist(\"Counts\", dataset_axis, pt_axis),\n",
    "            \"met_Higgs_CR\":     hist.Hist(\"Counts\", dataset_axis, pt_axis),\n",
    "            \"met_W_CR\":         hist.Hist(\"Counts\", dataset_axis, pt_axis),\n",
    "            \"met_Higgs_W_CR\":   hist.Hist(\"Counts\", dataset_axis, pt_axis),\n",
    "            \n",
    "            \"ht_CR\":            hist.Hist(\"Counts\", dataset_axis, ht_axis),\n",
    "            \"ht_Higgs_CR\":      hist.Hist(\"Counts\", dataset_axis, ht_axis),\n",
    "            \"ht_W_CR\":          hist.Hist(\"Counts\", dataset_axis, ht_axis),\n",
    "            \"ht_Higgs_W_CR\":    hist.Hist(\"Counts\", dataset_axis, ht_axis),\n",
    "            \n",
    "            \"N_AK8_CR\" :        hist.Hist(\"Counts\", dataset_axis, multiplicity_axis),\n",
    "            \"W_pt_CR\" :         hist.Hist(\"Counts\", dataset_axis, pt_axis),\n",
    "            \"H_pt_CR\" :         hist.Hist(\"Counts\", dataset_axis, pt_axis),\n",
    "            \"W_eta_CR\" :        hist.Hist(\"Counts\", dataset_axis, eta_axis),\n",
    "            \"H_eta_CR\" :        hist.Hist(\"Counts\", dataset_axis, eta_axis),\n",
    "            \n",
    "            \"N_AK8_Higgs_CR\" :  hist.Hist(\"Counts\", dataset_axis, multiplicity_axis),\n",
    "            \"W_pt_Higgs_CR\" :   hist.Hist(\"Counts\", dataset_axis, pt_axis),\n",
    "            \"H_pt_Higgs_CR\" :   hist.Hist(\"Counts\", dataset_axis, pt_axis),\n",
    "            \"W_eta_Higgs_CR\" :  hist.Hist(\"Counts\", dataset_axis, eta_axis),\n",
    "            \"H_eta_Higgs_CR\" :  hist.Hist(\"Counts\", dataset_axis, eta_axis),\n",
    "            \n",
    "            \"N_AK8_W_CR\" :      hist.Hist(\"Counts\", dataset_axis, multiplicity_axis),\n",
    "            \"W_pt_W_CR\" :       hist.Hist(\"Counts\", dataset_axis, pt_axis),\n",
    "            \"H_pt_W_CR\" :       hist.Hist(\"Counts\", dataset_axis, pt_axis),\n",
    "            \"W_eta_W_CR\" :      hist.Hist(\"Counts\", dataset_axis, eta_axis),\n",
    "            \"H_eta_W_CR\" :      hist.Hist(\"Counts\", dataset_axis, eta_axis),\n",
    "            \n",
    "            \"N_AK8_Higgs_W_CR\" :hist.Hist(\"Counts\", dataset_axis, multiplicity_axis),\n",
    "            \"W_pt_Higgs_W_CR\" : hist.Hist(\"Counts\", dataset_axis, pt_axis),\n",
    "            \"H_pt_Higgs_W_CR\" : hist.Hist(\"Counts\", dataset_axis, pt_axis),\n",
    "            \"W_eta_Higgs_W_CR\" :hist.Hist(\"Counts\", dataset_axis, eta_axis),\n",
    "            \"H_eta_Higgs_W_CR\" :hist.Hist(\"Counts\", dataset_axis, eta_axis),\n",
    "            \n",
    "            \"lead_AK8_pt\" :     hist.Hist(\"Counts\", dataset_axis, pt_axis),\n",
    "            \"sublead_AK8_pt\" :  hist.Hist(\"Counts\", dataset_axis, pt_axis),\n",
    "            \"lead_AK8_eta\" :    hist.Hist(\"Counts\", dataset_axis, eta_axis),\n",
    "            \"sublead_AK8_eta\" : hist.Hist(\"Counts\", dataset_axis, eta_axis),\n",
    "            \"lead_AK8_mass\" :   hist.Hist(\"Counts\", dataset_axis, mass_axis),\n",
    "            \"sublead_AK8_mass\" : hist.Hist(\"Counts\", dataset_axis, mass_axis),\n",
    "            \"N_W_CR\" :          hist.Hist(\"Counts\", dataset_axis, multiplicity_axis),\n",
    "            \"N_H_CR\" :          hist.Hist(\"Counts\", dataset_axis, multiplicity_axis),\n",
    "            \"N_AK4_CR\" :        hist.Hist(\"Counts\", dataset_axis, multiplicity_axis),\n",
    "\n",
    "\n",
    "            #\"N_b\" :             hist.Hist(\"Counts\", dataset_axis, multiplicity_axis),\n",
    "            #\"N_AK4\" :           hist.Hist(\"Counts\", dataset_axis, multiplicity_axis),\n",
    "            #\"N_AK8\" :           hist.Hist(\"Counts\", dataset_axis, multiplicity_axis),\n",
    "            #\"N_H\" :             hist.Hist(\"Counts\", dataset_axis, multiplicity_axis),\n",
    "            #\"N_W\" :             hist.Hist(\"Counts\", dataset_axis, multiplicity_axis),\n",
    "            \n",
    "            #\"WH_deltaPhi\":      hist.Hist(\"Counts\", dataset_axis, delta_axis),\n",
    "            #\"WH_deltaR\":        hist.Hist(\"Counts\", dataset_axis, delta_axis),\n",
    "            #\"bb_deltaPhi\":      hist.Hist(\"Counts\", dataset_axis, delta_axis),\n",
    "            #\"bb_deltaR\":        hist.Hist(\"Counts\", dataset_axis, delta_axis),\n",
    "            #\"min_dphiJetMet4\":  hist.Hist(\"Counts\", dataset_axis, delta_axis),\n",
    "            #\"dphiDiJet\":        hist.Hist(\"Counts\", dataset_axis, delta_axis),\n",
    "            #\"dphiDiFatJet\":     hist.Hist(\"Counts\", dataset_axis, delta_axis),\n",
    "            \n",
    "            'mC750_l1':         processor.defaultdict_accumulator(int),\n",
    "            'WJets':            processor.defaultdict_accumulator(int),\n",
    "            'WJetsOld':         processor.defaultdict_accumulator(int),\n",
    "            'WJetsNew':         processor.defaultdict_accumulator(int),\n",
    "            'QCD':              processor.defaultdict_accumulator(int),\n",
    "            'TTJets':           processor.defaultdict_accumulator(int),\n",
    "            'ttbarOld':         processor.defaultdict_accumulator(int),\n",
    "            'ttbarNew':         processor.defaultdict_accumulator(int),\n",
    "            'ZNuNu':            processor.defaultdict_accumulator(int),\n",
    "            'ST':               processor.defaultdict_accumulator(int),\n",
    "            'ST_tW':            processor.defaultdict_accumulator(int),\n",
    "            'ST_tChannel':      processor.defaultdict_accumulator(int),\n",
    "            'ST_sChannel':      processor.defaultdict_accumulator(int),\n",
    "            'ttW':              processor.defaultdict_accumulator(int),\n",
    "            'ttZ':              processor.defaultdict_accumulator(int),\n",
    "            'WW':               processor.defaultdict_accumulator(int),\n",
    "            'WZ/ZZ':            processor.defaultdict_accumulator(int),\n",
    "            'LL':               processor.defaultdict_accumulator(int),\n",
    "            'Data':             processor.defaultdict_accumulator(int),\n",
    "            'totalEvents':      processor.defaultdict_accumulator(int),\n",
    "            'test1':            processor.defaultdict_accumulator(float),\n",
    "        })\n",
    "\n",
    "    @property\n",
    "    def accumulator(self):\n",
    "        return self._accumulator\n",
    "\n",
    "    def process(self, df):\n",
    "        \"\"\"\n",
    "        Processing function. This is where the actual analysis happens.\n",
    "        \"\"\"\n",
    "        output = self.accumulator.identity()\n",
    "        dataset = df[\"dataset\"]\n",
    "        cfg = loadConfig()\n",
    "        \n",
    "        ## MET -> can switch to puppi MET\n",
    "        met_pt  = df[\"MET_pt\"]\n",
    "        met_phi = df[\"MET_phi\"]\n",
    "        \n",
    "        ## Load Objects\n",
    "        muon     = getMuons(df, WP='veto')\n",
    "        electron = getElectrons(df, WP='veto')\n",
    "        fatjet   = getFatJets(df)\n",
    "        jet      = getJets(df)\n",
    "        triggers = getTriggers(df, year=2018, dataset=dataset) #hard-coding the year since I'm only looking at one, will need to change!!!!!\n",
    "        filters  = getFilters(df, year=2018, dataset=dataset) #hard-coding the year since I'm only looking at one, will need to change!!!!!\n",
    "        \n",
    "        ## Clean Objects\n",
    "        skimjet   = jet[(jet.pt>30) & (abs(jet.eta)<2.4)]\n",
    "        jet       = jet[(jet.pt>30) & (jet.jetId>1) & (abs(jet.eta)<2.4)]\n",
    "        jet       = jet[~jet.match(muon, deltaRCut=0.4)] # remove jets that overlap with muons\n",
    "        jet       = jet[~jet.match(electron, deltaRCut=0.4)] # remove jets that overlap with electrons\n",
    "        jet       = jet[~jet.match(fatjet, deltaRCut=1.2)] # remove AK4 jets that overlap with AK8 jets\n",
    "        jet       = jet[jet.pt.argsort(ascending=False)] # sort the jets\n",
    "        btag      = jet[(jet.btagDeepB>0.4184)]\n",
    "        light     = jet[(jet.btagDeepB<0.4184)]\n",
    "        \n",
    "        ## Calculating Variables\n",
    "        \n",
    "        ## FatJet Variables\n",
    "        leadFatJet = fatjet[:,:1]\n",
    "        leadingFatJets = fatjet[:,:2]\n",
    "        difatjet = leadingFatJets.choose(2)\n",
    "        subleadFatJet = leadingFatJets[leadingFatJets.pt.argmin()]\n",
    "        dphiDiFatJet = np.arccos(np.cos(difatjet.i0.phi-difatjet.i1.phi))\n",
    "        \n",
    "        ## H-tagged Variables\n",
    "        htag = fatjet[((fatjet.pt > 200) & (fatjet.deepTagMD_HbbvsQCD > 0.8365))]\n",
    "        htag_hard = fatjet[((fatjet.pt > 300) & (fatjet.deepTagMD_HbbvsQCD > 0.8365))]\n",
    "        \n",
    "        lead_htag = htag[htag.pt.argmax()]\n",
    "        \n",
    "        ## W-tagged Variables\n",
    "        wtag = fatjet[((fatjet.pt > 200) & (fatjet.deepTagMD_HbbvsQCD < 0.8365) & (fatjet.deepTag_WvsQCD > 0.918))]\n",
    "        wtag_hard = fatjet[((fatjet.pt > 300) & (fatjet.deepTagMD_HbbvsQCD < 0.8365) & (fatjet.deepTag_WvsQCD > 0.918))]\n",
    "        \n",
    "        lead_wtag = wtag[wtag.pt.argmax()]\n",
    "        \n",
    "        ## W, H tagged variables\n",
    "        wh = lead_htag.cross(lead_wtag)\n",
    "        wh_deltaPhi = np.arccos(wh.i0.phi - wh.i1.phi)\n",
    "        wh_deltaR = wh.i0.p4.delta_r(wh.i1.p4)\n",
    "        \n",
    "        ## b-tagged variables\n",
    "        high_score_btag = jet[jet.btagDeepB.argsort(ascending=False)][:,:2]\n",
    "        \n",
    "        leading_jet    = jet[jet.pt.argmax()]\n",
    "        leading_b      = btag[btag.pt.argmax()]\n",
    "        \n",
    "        bb = high_score_btag.choose(2)\n",
    "        bb_deltaPhi = np.arccos(np.cos(bb.i0.phi-bb.i1.phi))\n",
    "        bb_deltaR = bb.i0.p4.delta_r(bb.i1.p4)\n",
    "        \n",
    "        ## mt variabels\n",
    "        mtb = mt(btag.pt, btag.phi, met_pt, met_phi)\n",
    "        min_mtb = mtb.min()\n",
    "        mth = mt(htag.pt, htag.phi, met_pt, met_phi)\n",
    "\n",
    "        ## other variables\n",
    "        ht = fatjet.pt.sum()\n",
    "        \n",
    "        min_dphiJetMet4 = np.arccos(np.cos(jet[:,:4].phi-met_phi)).min()\n",
    "        \n",
    "        leadingJets = jet[:,:2]\n",
    "        dijet = leadingJets.choose(2)\n",
    "        dphiDiJet = np.arccos(np.cos(dijet.i0.phi-dijet.i1.phi))\n",
    "        \n",
    "        min_dphiFatJetMet4 = np.arccos(np.cos(fatjet[:,:4].phi-met_phi)).min()\n",
    "\n",
    "        ## variables for selections       \n",
    "        wtag_sel = ( wtag.counts>0 & (abs(wtag.msoftdrop-80)<30).any())\n",
    "        htag_sel = ( htag.counts>0 & (abs(htag.msoftdrop-125)<25).any())\n",
    "        stitchVar = 1 if dataset=='Data' else df[\"stitch\"]\n",
    "        \n",
    "        ## define selections (maybe move to a different file at some point)\n",
    "        \n",
    "        output['totalEvents']['all'] += len(df['weight'])\n",
    "        \n",
    "        # Cutflow\n",
    "        processes = processesList\n",
    "        weight = np.ones(len(df['weight'])) if dataset=='Data' else df['weight']\n",
    "        lumi = 1 if dataset=='Data' else 60\n",
    "        fullweight = weight * lumi\n",
    "        \n",
    "        cutflow = Cutflow(output, df, cfg, processes, weight=fullweight)\n",
    "        \n",
    "        cutflow.addRow( 'filters',   (filters) )\n",
    "        cutflow.addRow( 'triggers',   (triggers) )\n",
    "        \n",
    "        #cutflow.addRow( 'stitch',   (stitchVar ==1) )\n",
    "        \n",
    "        cutflow.addRow( 'skim',   ((met_pt>200) & (skimjet.counts>1)) )\n",
    "        cutflow.addRow( 'Exactly 1 e or mu',   ((electron.counts+muon.counts)==1) )\n",
    "        cutflow.addRow( 'MET>250',     (met_pt>250) )\n",
    "        \n",
    "        baseline = copy.deepcopy(cutflow.selection)\n",
    "        \n",
    "        cutflow.addRow( 'N_fatjet>1',      (fatjet.counts>1) )\n",
    "        cutflow.addRow( 'min_dphiFatJetMet4', (min_dphiFatJetMet4>0.5))\n",
    "        cutflow.addRow( 'dphiDiFatJet', (dphiDiFatJet<2.5).all() ) # by using .all() I do not implicitely cut on the number of fat jets\n",
    "        cutflow.addRow( 'minmth>200',   (mth.min()>200) )\n",
    "        cutflow.addRow( 'njet veto',     (jet.counts<2))\n",
    "\n",
    "        vetoQCD = copy.deepcopy(cutflow.selection)\n",
    "        \n",
    "        cutflow.addRow( 'N_wtag>0',     (wtag_sel), cumulative=False)\n",
    "        \n",
    "        wtag_selection = copy.deepcopy(cutflow.selection)\n",
    "        \n",
    "        cutflow.addRow( 'N_htag>0',     (htag_sel), cumulative=False)\n",
    "\n",
    "        htag_selection = copy.deepcopy(cutflow.selection)\n",
    "        \n",
    "        cutflow.addRow( 'N_htag>0, N_wtag>0',     (htag_sel & wtag_sel))\n",
    "\n",
    "        signal_selection = cutflow.selection\n",
    "        \n",
    "        ### And fill the histograms\n",
    "        output['met_CR'].fill(dataset=dataset, pt=met_pt[vetoQCD].flatten(), weight=fullweight[vetoQCD])\n",
    "        output['met_W_CR'].fill(dataset=dataset, pt=met_pt[vetoQCD & wtag_sel].flatten(), weight=fullweight[vetoQCD & wtag_sel])\n",
    "        output['met_Higgs_CR'].fill(dataset=dataset, pt=met_pt[vetoQCD & htag_sel].flatten(), weight=fullweight[vetoQCD & htag_sel])\n",
    "        output['met_Higgs_W_CR'].fill(dataset=dataset, pt=met_pt[signal_selection].flatten(), weight=fullweight[signal_selection])\n",
    "\n",
    "        output['ht_CR'].fill(dataset=dataset, ht=ht[vetoQCD].flatten(), weight=fullweight[vetoQCD])\n",
    "        output['ht_W_CR'].fill(dataset=dataset, ht=ht[vetoQCD & wtag_sel].flatten(), weight=fullweight[vetoQCD & wtag_sel])\n",
    "        output['ht_Higgs_CR'].fill(dataset=dataset, ht=ht[vetoQCD & htag_sel].flatten(), weight=fullweight[vetoQCD & htag_sel])\n",
    "        output['ht_Higgs_W_CR'].fill(dataset=dataset, ht=ht[signal_selection].flatten(), weight=fullweight[signal_selection])\n",
    "        \n",
    "        output['N_AK8_CR'].fill(dataset=dataset, multiplicity=fatjet[vetoQCD].counts, weight=fullweight[vetoQCD])\n",
    "        #output['W_pt_CR'].fill(dataset=dataset, pt=lead_wtag[vetoQCD].pt.flatten(), weight=fullweight[vetoQCD])\n",
    "        #output['H_pt_CR'].fill(dataset=dataset, pt=lead_htag[vetoQCD].pt.flatten(), weight=fullweight[vetoQCD])\n",
    "        #output['W_eta_CR'].fill(dataset=dataset, eta=lead_wtag[vetoQCD].eta.flatten(), weight=fullweight[vetoQCD])\n",
    "        #output['H_eta_CR'].fill(dataset=dataset, eta=lead_htag[vetoQCD].eta.flatten(), weight=fullweight[vetoQCD])\n",
    "\n",
    "        output['lead_AK8_pt'].fill(dataset=dataset, pt=leadFatJet[vetoQCD].pt.flatten(), weight=fullweight[vetoQCD])\n",
    "        output['sublead_AK8_pt'].fill(dataset=dataset, pt=subleadFatJet[vetoQCD].pt.flatten(), weight=fullweight[vetoQCD])\n",
    "        output['lead_AK8_eta'].fill(dataset=dataset, eta=leadFatJet[vetoQCD].eta.flatten(), weight=fullweight[vetoQCD])\n",
    "        output['sublead_AK8_eta'].fill(dataset=dataset, eta=subleadFatJet[vetoQCD].eta.flatten(), weight=fullweight[vetoQCD])\n",
    "        output['lead_AK8_mass'].fill(dataset=dataset, mass=leadFatJet[vetoQCD].msoftdrop.flatten(), weight=fullweight[vetoQCD])\n",
    "        output['sublead_AK8_mass'].fill(dataset=dataset, mass=subleadFatJet[vetoQCD].msoftdrop.flatten(), weight=fullweight[vetoQCD])\n",
    "        output['N_W_CR'].fill(dataset=dataset, multiplicity=wtag[vetoQCD].counts, weight=fullweight[vetoQCD])\n",
    "        output['N_H_CR'].fill(dataset=dataset, multiplicity=htag[vetoQCD].counts, weight=fullweight[vetoQCD])\n",
    "        output['N_AK4_CR'].fill(dataset=dataset, multiplicity=jet[vetoQCD].counts, weight=fullweight[vetoQCD])\n",
    "                \n",
    "        output['N_AK8_W_CR'].fill(dataset=dataset, multiplicity=fatjet[vetoQCD & wtag_sel].counts, weight=fullweight[vetoQCD & wtag_sel])\n",
    "        output['W_pt_W_CR'].fill(dataset=dataset, pt=lead_wtag[vetoQCD & wtag_sel].pt.flatten(), weight=fullweight[vetoQCD & wtag_sel])\n",
    "        #output['H_pt_W_CR'].fill(dataset=dataset, pt=lead_htag[vetoQCD & wtag_sel].pt.flatten(), weight=fullweight[vetoQCD & wtag_sel])\n",
    "        output['W_eta_W_CR'].fill(dataset=dataset, eta=lead_wtag[vetoQCD & wtag_sel].eta.flatten(), weight=fullweight[vetoQCD & wtag_sel])\n",
    "        #output['H_eta_W_CR'].fill(dataset=dataset, eta=lead_htag[vetoQCD & wtag_sel].eta.flatten(), weight=fullweight[vetoQCD & wtag_sel])\n",
    "\n",
    "        output['N_AK8_Higgs_CR'].fill(dataset=dataset, multiplicity=fatjet[vetoQCD & htag_sel].counts, weight=fullweight[vetoQCD & htag_sel])\n",
    "        #output['W_pt_Higgs_CR'].fill(dataset=dataset, pt=lead_wtag[vetoQCD & htag_sel].pt.flatten(), weight=fullweight[vetoQCD & htag_sel])\n",
    "        output['H_pt_Higgs_CR'].fill(dataset=dataset, pt=lead_htag[vetoQCD & htag_sel].pt.flatten(), weight=fullweight[vetoQCD & htag_sel])\n",
    "        #output['W_eta_Higgs_CR'].fill(dataset=dataset, eta=lead_wtag[vetoQCD & htag_sel].eta.flatten(), weight=fullweight[vetoQCD & htag_sel])\n",
    "        output['H_eta_Higgs_CR'].fill(dataset=dataset, eta=lead_htag[vetoQCD & htag_sel].eta.flatten(), weight=fullweight[vetoQCD & htag_sel])\n",
    "\n",
    "        output['N_AK8_Higgs_W_CR'].fill(dataset=dataset, multiplicity=fatjet[signal_selection].counts, weight=fullweight[signal_selection])\n",
    "        output['W_pt_Higgs_W_CR'].fill(dataset=dataset, pt=lead_wtag[signal_selection].pt.flatten(), weight=fullweight[signal_selection])\n",
    "        output['H_pt_Higgs_W_CR'].fill(dataset=dataset, pt=lead_htag[signal_selection].pt.flatten(), weight=fullweight[signal_selection])\n",
    "        output['W_eta_Higgs_W_CR'].fill(dataset=dataset, eta=lead_wtag[signal_selection].eta.flatten(), weight=fullweight[signal_selection])\n",
    "        output['H_eta_Higgs_W_CR'].fill(dataset=dataset, eta=lead_htag[signal_selection].eta.flatten(), weight=fullweight[signal_selection])\n",
    "\n",
    "                \n",
    "        '''output['MET_pt_baseline'].fill(dataset=dataset, pt=met_pt[baseline].flatten(), weight=df['weight'][baseline]*cfg['lumi'])\n",
    "        output['HT_baseline'].fill(dataset=dataset, ht=ht[baseline].flatten(), weight=df['weight'][baseline]*cfg['lumi'])\n",
    "        output['mtb_min_baseline'].fill(dataset=dataset, mass=mtb[baseline].min().flatten(), weight=df['weight'][baseline]*cfg['lumi'])\n",
    "\n",
    "        output['MET_pt'].fill(dataset=dataset, pt=met_pt[vetoQCD].flatten(), weight=df['weight'][vetoQCD]*cfg['lumi'])\n",
    "        output['HT'].fill(dataset=dataset, ht=ht[vetoQCD].flatten(), weight=df['weight'][vetoQCD]*cfg['lumi'])\n",
    "        output['mtb_min'].fill(dataset=dataset, mass=mtb[vetoQCD].min().flatten(), weight=df['weight'][vetoQCD]*cfg['lumi'])\n",
    "        \n",
    "        ## N jet and N b without selections on those\n",
    "        output['N_AK4'].fill(dataset=dataset, multiplicity=jet[baseline].counts, weight=df['weight'][baseline]*cfg['lumi'])\n",
    "        output['N_b'].fill(dataset=dataset, multiplicity=btag[baseline].counts, weight=df['weight'][baseline]*cfg['lumi'])       \n",
    "        output['N_W'].fill(dataset=dataset, multiplicity=htag[baseline].counts, weight=df['weight'][baseline]*cfg['lumi'])       \n",
    "        output['N_H'].fill(dataset=dataset, multiplicity=wtag[baseline].counts, weight=df['weight'][baseline]*cfg['lumi'])       \n",
    "        output['N_AK8'].fill(dataset=dataset, multiplicity=fatjet[baseline].counts, weight=df['weight'][baseline]*cfg['lumi'])       \n",
    "\n",
    "        #output['bb_deltaPhi'].fill(dataset=dataset, delta=bb_deltaPhi[baseline].flatten(), weight=df['weight'][baseline]*cfg['lumi'])\n",
    "        #output['bb_deltaR'].fill(dataset=dataset, delta=bb_deltaR[baseline].flatten(), weight=df['weight'][baseline]*cfg['lumi'])\n",
    "\n",
    "        output['min_dphiJetMet4'].fill(dataset=dataset, delta=min_dphiJetMet4[baseline].flatten(), weight=df['weight'][baseline]*cfg['lumi'])\n",
    "        output['dphiDiJet'].fill(dataset=dataset, delta=dphiDiJet[baseline].min().flatten(), weight=df['weight'][baseline]*cfg['lumi'])\n",
    "\n",
    "        ## Higgs and W pt\n",
    "        output['lead_AK8_pt'].fill(dataset=dataset, pt=fatjet[(baseline & (fatjet.counts>0))].pt.max().flatten(), weight=df['weight'][(baseline & (fatjet.counts>0))]*cfg['lumi'])\n",
    "        output['dphiDiFatJet'].fill(dataset=dataset, delta=dphiDiFatJet[(baseline & (fatjet.counts>1))].min().flatten(), weight=df['weight'][(baseline & (fatjet.counts>1))]*cfg['lumi'])\n",
    "\n",
    "        output['H_pt'].fill(dataset=dataset, pt=lead_htag[event_selection].pt.flatten(), weight=df['weight'][event_selection]*cfg['lumi'])\n",
    "        output['H_eta'].fill(dataset=dataset, eta=lead_htag[event_selection].eta.flatten(), weight=df['weight'][event_selection]*cfg['lumi'])\n",
    "\n",
    "        output['W_pt'].fill(dataset=dataset, pt=lead_wtag[event_selection].pt.flatten(), weight=df['weight'][event_selection]*cfg['lumi'])\n",
    "        output['W_eta'].fill(dataset=dataset, eta=lead_wtag[event_selection].eta.flatten(), weight=df['weight'][event_selection]*cfg['lumi'])\n",
    "\n",
    "        output['WH_deltaPhi'].fill(dataset=dataset, delta=wh_deltaPhi[event_selection].flatten(), weight=df['weight'][event_selection]*cfg['lumi'])\n",
    "        output['WH_deltaR'].fill(dataset=dataset, delta=wh_deltaR[event_selection].flatten(), weight=df['weight'][event_selection]*cfg['lumi'])\n",
    "\n",
    "        output['MET_pt_CR'].fill(dataset=dataset, pt=met_pt[event_selection].flatten(), weight=df['weight'][event_selection]*cfg['lumi'])\n",
    "        output['HT_CR'].fill(dataset=dataset, ht=ht[event_selection].flatten(), weight=df['weight'][event_selection]*cfg['lumi'])\n",
    "        output['mtb_min_CR'].fill(dataset=dataset, mass=mtb[event_selection].min().flatten(), weight=df['weight'][event_selection]*cfg['lumi'])\n",
    "        '''\n",
    "        return output\n",
    "\n",
    "    def postprocess(self, accumulator):\n",
    "        return accumulator"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "runLocal = True\n",
    "\n",
    "\n",
    "if not runLocal:\n",
    "    # Get the scheduler from the dask_cluster notebook\n",
    "    from dask.distributed import Client, progress\n",
    "\n",
    "    c = Client('tcp://169.228.130.5:27879')\n",
    "\n",
    "    ## for dask\n",
    "    exe_args = {\n",
    "        'client': c,\n",
    "        #'savemetrics': True,\n",
    "    }\n",
    "    exe = processor.dask_executor\n",
    "    \n",
    "else:\n",
    "    ## for local\n",
    "    exe_args = {\n",
    "        'workers': 4,\n",
    "        'function_args': {'flatten': False}\n",
    "    }\n",
    "    exe = processor.futures_executor\n",
    "\n",
    "if not runLocal:\n",
    "    print(c)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "e7269217c68242a09bdaf5c92bd615b0",
       "version_major": 2,
       "version_minor": 0
      },
      "text/html": [
       "<p>Failed to display Jupyter Widget of type <code>HBox</code>.</p>\n",
       "<p>\n",
       "  If you're reading this message in the Jupyter Notebook or JupyterLab Notebook, it may mean\n",
       "  that the widgets JavaScript is still loading. If this message persists, it\n",
       "  likely means that the widgets JavaScript library is either not installed or\n",
       "  not enabled. See the <a href=\"https://ipywidgets.readthedocs.io/en/stable/user_install.html\">Jupyter\n",
       "  Widgets Documentation</a> for setup instructions.\n",
       "</p>\n",
       "<p>\n",
       "  If you're reading this message in another frontend (for example, a static\n",
       "  rendering on GitHub or <a href=\"https://nbviewer.jupyter.org/\">NBViewer</a>),\n",
       "  it may mean that your frontend doesn't currently support widgets.\n",
       "</p>\n"
      ],
      "text/plain": [
       "HBox(children=(HTML(value='Preprocessing'), FloatProgress(value=0.0, max=876.0), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "overwrite = True\n",
    "small = False\n",
    "\n",
    "tag = 'mbryson/WH_hadronic/v0.2.4'\n",
    "sigtag = 'ksalyer/allHadTest/0p1p27'\n",
    "#tag = 'ksalyer/allHadTest/0p1p16/2018'\n",
    "\n",
    "fileset_WH   = {'mC750_l1': glob.glob('/hadoop/cms/store/user/'+sigtag+'/WH_had_750_1_nanoAOD/*.root'),\n",
    "                'WJets': glob.glob('/hadoop/cms/store/user/'+tag+'/W*JetsToLNu_Tune*/*.root'),\n",
    "                'QCD': glob.glob('/hadoop/cms/store/user/'+tag+'/QCD_HT*/*.root'),\n",
    "                'TTJets': glob.glob('/hadoop/cms/store/user/'+tag+'/TTJets*/*.root'),\n",
    "                'ZNuNu': glob.glob('/hadoop/cms/store/user/'+tag+'/ZJetsToNuNu*/*.root'),\n",
    "                'ST': glob.glob('/hadoop/cms/store/user/'+tag+'/ST*/*.root'),\n",
    "                #'ST_tW': glob.glob('/hadoop/cms/store/user/'+tag+'/ST_tW*/*.root'),\n",
    "                #'ST_tChannel': glob.glob('/hadoop/cms/store/'+tag+'/ST_t-channel*/*.root'),\n",
    "                #'ST_sChannel': glob.glob('/hadoop/cms/store/user/'+tag+'/ST_s-channel*/*.root'),\n",
    "                'ttW': glob.glob('/hadoop/cms/store/user/'+tag+'/ttWJets*/*.root'),\n",
    "                'ttZ': glob.glob('/hadoop/cms/store/user/'+tag+'/ttZJets*/*.root'),\n",
    "                'WW': glob.glob('/hadoop/cms/store/user/'+tag+'/WW*/*.root'),\n",
    "                'WZ/ZZ': glob.glob('/hadoop/cms/store/user/'+tag+'/WZ*/*.root')\n",
    "                    +glob.glob('/hadoop/cms/store/user/'+tag+'/ZZTo2L2Nu*/*.root')\n",
    "                    +glob.glob('/hadoop/cms/store/user/'+tag+'/ZZTo2Q2Nu*/*.root'),\n",
    "                'Data': glob.glob('/hadoop/cms/store/user/'+tag+'/MET_Run2018*/*.root')\n",
    "                }\n",
    "\n",
    "fileset_LL   = {'mC750_l1': glob.glob('/hadoop/cms/store/user/'+sigtag+'/WH_had_750_1_nanoAOD/*.root'),\n",
    "                'WJets': glob.glob('/hadoop/cms/store/user/'+tag+'/W*JetsToLNu_Tune*/*.root'),\n",
    "                        #+glob.glob('/hadoop/cms/store/user/'+tag+'/W*JetsToLNu_NuPt*/*.root'),\n",
    "                'TTJets': glob.glob('/hadoop/cms/store/user/'+tag+'/TTJets_DiLept_Tune*/*.root')\n",
    "                         +glob.glob('/hadoop/cms/store/user/'+tag+'/TTJets_SingleLeptFromT_Tune*/*.root')\n",
    "                         +glob.glob('/hadoop/cms/store/user/'+tag+'/TTJets_SingleLeptFromTbar_Tune*/*.root'),\n",
    "                'ST': glob.glob('/hadoop/cms/store/user/'+tag+'/ST*/*.root'),\n",
    "                'ttW': glob.glob('/hadoop/cms/store/user/'+tag+'/ttWJets*/*.root'),\n",
    "                'WW': glob.glob('/hadoop/cms/store/user/'+tag+'/WW*/*.root'),\n",
    "                'ZNuNu': glob.glob('/hadoop/cms/store/user/'+tag+'/ZJetsToNuNu*/*.root')\n",
    "                        +glob.glob('/hadoop/cms/store/user/'+tag+'/ttZJets*/*.root')\n",
    "                        +glob.glob('/hadoop/cms/store/user/'+tag+'/WZ*/*.root')\n",
    "                        +glob.glob('/hadoop/cms/store/user/'+tag+'/ZZTo2L2Nu*/*.root')\n",
    "                        +glob.glob('/hadoop/cms/store/user/'+tag+'/ZZTo2Q2Nu*/*.root'),\n",
    "                'QCD': glob.glob('/hadoop/cms/store/user/'+tag+'/QCD_HT*/*.root'),\n",
    "                'Data': glob.glob('/hadoop/cms/store/user/'+tag+'/MET_Run2018*/*.root')\n",
    "                }\n",
    "\n",
    "fileset_wjets = {'mC750_l1': glob.glob('/hadoop/cms/store/user/'+sigtag+'/WH_had_750_1_nanoAOD/*.root'),\n",
    "                 'WJetsOld': glob.glob('/hadoop/cms/store/user/'+sigtag+'/W*JetsToLNu_Tune*/*.root'),\n",
    "                 'WJetsNew': glob.glob('/hadoop/cms/store/user/'+tag+'/W*JetsToLNu_Tune*/*.root'),\n",
    "                }\n",
    "\n",
    "fileset_ttbar = {'mC750_l1': glob.glob('/hadoop/cms/store/user/'+sigtag+'/WH_had_750_1_nanoAOD/*.root'),\n",
    "                 'ttbarOld': glob.glob('/hadoop/cms/store/user/'+sigtag+'/TTJets_DiLept_Tune*/*.root')\n",
    "                            +glob.glob('/hadoop/cms/store/user/'+sigtag+'/TTJets_SingleLeptFromT_Tune*/*.root')\n",
    "                            +glob.glob('/hadoop/cms/store/user/'+sigtag+'/TTJets_SingleLeptFromTbar_Tune*/*.root'),\n",
    "                 'ttbarNew': glob.glob('/hadoop/cms/store/user/'+tag+'/TTJets_DiLept_Tune*/*.root')\n",
    "                            +glob.glob('/hadoop/cms/store/user/'+tag+'/TTJets_SingleLeptFromT_Tune*/*.root')\n",
    "                            +glob.glob('/hadoop/cms/store/user/'+tag+'/TTJets_SingleLeptFromTbar_Tune*/*.root'),\n",
    "                }\n",
    "\n",
    "fileset_WH_merge = {'mC750_l1': glob.glob('/hadoop/cms/store/user/'+sigtag+'/WH_had_750_1_nanoAOD/*.root'),\n",
    "                'LL': glob.glob('/hadoop/cms/store/user/'+tag+'/W*JetsToLNu_Tune*/*.root')\n",
    "                    + glob.glob('/hadoop/cms/store/user/'+tag+'/TTJets*/*.root')\n",
    "                    + glob.glob('/hadoop/cms/store/user/'+tag+'/ST*/*.root')\n",
    "                    + glob.glob('/hadoop/cms/store/user/'+tag+'/ttWJets*/*.root')\n",
    "                    + glob.glob('/hadoop/cms/store/user/'+tag+'/WW*/*.root'),\n",
    "                'QCD': glob.glob('/hadoop/cms/store/user/'+tag+'/QCD_HT*/*.root'),\n",
    "                'ZNuNu': glob.glob('/hadoop/cms/store/user/'+tag+'/ZJetsToNuNu*/*.root')\n",
    "                    + glob.glob('/hadoop/cms/store/user/'+tag+'/ttZJets*/*.root')\n",
    "                    + glob.glob('/hadoop/cms/store/user/'+tag+'/WZ*/*.root')\n",
    "                    + glob.glob('/hadoop/cms/store/user/'+tag+'/ZZTo2L2Nu*/*.root')\n",
    "                    + glob.glob('/hadoop/cms/store/user/'+tag+'/ZZTo2Q2Nu*/*.root'),\n",
    "                'Data': glob.glob('/hadoop/cms/store/user/'+tag+'/MET_Run2018*/*.root')\n",
    "\n",
    "                }\n",
    "\n",
    "fileset_smLL = {'mC750_l1': glob.glob('/hadoop/cms/store/user/'+sigtag+'/WH_had_750_1_nanoAOD/*.root'),\n",
    "                'WJets': glob.glob('/hadoop/cms/store/user/'+tag+'/W*JetsToLNu_Tune*/*.root')[:2],\n",
    "                'TTJets': glob.glob('/hadoop/cms/store/user/'+tag+'/TTJets*/*.root')[:2],\n",
    "                'ST': glob.glob('/hadoop/cms/store/user/'+tag+'/ST*/*.root')[:2],\n",
    "                'ttW': glob.glob('/hadoop/cms/store/user/'+tag+'/ttWJets*/*.root')[:2],\n",
    "                'WW': glob.glob('/hadoop/cms/store/user/'+tag+'/WW*/*.root')[:2],\n",
    "                'ZNuNu': glob.glob('/hadoop/cms/store/user/'+tag+'/ZJetsToNuNu*/*.root')[:2],\n",
    "                'QCD': glob.glob('/hadoop/cms/store/user/'+tag+'/QCD_HT*/*.root')[:2],\n",
    "                'Data': glob.glob('/hadoop/cms/store/user/'+tag+'/MET_Run2018*/*.root')[:2]\n",
    "                }\n",
    "\n",
    "\n",
    "# load the config and the cache\n",
    "cfg = loadConfig()\n",
    "\n",
    "cacheName = 'WH_small' if small else 'WH'\n",
    "\n",
    "# histograms\n",
    "histograms = []\n",
    "histograms += ['N_AK4']\n",
    "\n",
    "# initialize cache\n",
    "cache = dir_archive(os.path.join(os.path.expandvars(cfg['caches']['base']), cfg['caches'][cacheName]), serialized=True)\n",
    "if not overwrite:\n",
    "    cache.load()\n",
    "\n",
    "if cfg == cache.get('cfg') and histograms == cache.get('histograms') and cache.get('simple_output'):\n",
    "    output = cache.get('simple_output')\n",
    "\n",
    "else:\n",
    "    # Run the processor\n",
    "    if small:\n",
    "        fileset = fileset_ttbar\n",
    "        workers = 4\n",
    "    else:\n",
    "        fileset = fileset_LL\n",
    "        workers = 16\n",
    "    \n",
    "        \n",
    "    output = processor.run_uproot_job(fileset,\n",
    "                                      treename='Events',\n",
    "                                      processor_instance=analysisProcessor(),\n",
    "                                      executor=exe,\n",
    "                                      executor_args=exe_args,\n",
    "                                      #chunksize=250000,\n",
    "                                      chunksize=100000,\n",
    "                                     )\n",
    "    cache['fileset']        = fileset\n",
    "    cache['cfg']            = cfg\n",
    "    cache['histograms']     = histograms\n",
    "    cache['simple_output']  = output\n",
    "    cache.dump()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Cutflow\n",
    "from Tools.helpers import getCutFlowTable\n",
    "\n",
    "processes = processesList\n",
    "lines     = ['entry']\n",
    "lines    += linesList\n",
    "df        = getCutFlowTable(output, processes=processes, lines=lines, significantFigures=4, signal='mC750_l1')\n",
    "df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Efficiencies\n",
    "df = getCutFlowTable(output, processes=processes, lines=lines, significantFigures=3, absolute=False)\n",
    "df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%matplotlib inline\n",
    "import matplotlib\n",
    "import matplotlib.pyplot as plt\n",
    "from matplotlib.colors import LogNorm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from plots.helpers import *\n",
    "bins = {\\\n",
    "    'N_AK4':    {'axis': 'multiplicity',  'overflow':'over',  'bins': hist.Bin('multiplicity', r'$N_{AK4 jet}$', 6, -0.5, 5.5)},\n",
    "    'N_AK4_SR':    {'axis': 'multiplicity',  'overflow':'over',  'bins': hist.Bin('multiplicity', r'$N_{AK4 jet}$', 6, -0.5, 5.5)},\n",
    "    'N_AK8':    {'axis': 'multiplicity',  'overflow':'over',  'bins': hist.Bin('multiplicity', r'$N_{AK8 jet}$', 5, -0.5, 4.5)},\n",
    "    'N_b':      {'axis': 'multiplicity',  'overflow':'over',  'bins': hist.Bin('multiplicity', r'$N_{b-tag}$', 5, -0.5, 4.5)},\n",
    "    'N_H':      {'axis': 'multiplicity',  'overflow':'over',  'bins': hist.Bin('multiplicity', r'$N_{H-tag}$', 5, -0.5, 4.5)},\n",
    "    'N_W':      {'axis': 'multiplicity',  'overflow':'over',  'bins': hist.Bin('multiplicity', r'$N_{W-tag}$', 5, -0.5, 4.5)},\n",
    "\n",
    "    'MET_pt':   {'axis': 'pt',      'overflow':'over',  'bins': hist.Bin('pt', r'$p_{T}^{miss}\\ (GeV)$', 20, 0, 800)},\n",
    "    'MET_ptCoarse':   {'axis': 'pt',      'overflow':'over',  'bins': hist.Bin('pt', r'$p_{T}^{miss}\\ (GeV)$', 5, 200, 700)},\n",
    "    'HT':       {'axis': 'ht',      'overflow':'over',  'bins': hist.Bin('pt', r'$H_{T} (AK4 jets) \\ (GeV)$', 25, 0, 2000)},    \n",
    "    'HT_Coarse':       {'axis': 'ht',      'overflow':'over',  'bins': hist.Bin('pt', r'$H_{T} (AK4 jets) \\ (GeV)$', 5, 0, 500)},    \n",
    "    'W_pt':     {'axis': 'pt',      'overflow':'over',  'bins': hist.Bin('pt', r'$p_{T} (W-tag)$', 8, 200, 600)},\n",
    "    'W_eta':    {'axis': 'eta',     'overflow':'over',  'bins': hist.Bin('eta', r'$\\eta (W-tag)$', 15, -5.5, 5.5)},\n",
    "    'H_pt':     {'axis': 'pt',      'overflow':'over',  'bins': hist.Bin('pt', r'$p_{T} (H-tag)$', 8, 200, 600)},\n",
    "    'H_eta':    {'axis': 'eta',     'overflow':'over',  'bins': hist.Bin('eta', r'$\\eta (H-tag)$', 15, -5.5, 5.5)},\n",
    "\n",
    "    'dphiDiFatJet': {'axis': 'delta',          'overflow':'over',  'bins': hist.Bin('delta', r'$\\Delta \\phi (AK8)$', 30, 0, 3)},\n",
    "    'dphiDiJet':    {'axis': 'delta',          'overflow':'over',  'bins': hist.Bin('delta', r'$\\Delta \\phi (AK4)$', 30, 0, 3)},\n",
    "    'WH_deltaPhi':  {'axis': 'delta',          'overflow':'over',  'bins': hist.Bin('delta', r'$\\Delta \\phi (WH)$', 6, 0, 3)},\n",
    "    'WH_deltaR':    {'axis': 'delta',          'overflow':'over',  'bins': hist.Bin('delta', r'$\\Delta R (WH)$', 10, 0, 5)},\n",
    "    'bb_deltaPhi':  {'axis': 'delta',          'overflow':'over',  'bins': hist.Bin('delta', r'$\\Delta \\phi (bb)$', 30, 0, 3)},\n",
    "    'bb_deltaR':    {'axis': 'delta',          'overflow':'over',  'bins': hist.Bin('delta', r'$\\Delta R (bb)$', 10, 0, 5)},\n",
    "    'min_dphiJetMet4': {'axis': 'delta',          'overflow':'over',  'bins': hist.Bin('delta', r'$\\Delta \\phi (j, p_{T}^{miss})$', 30, 0, 3)},\n",
    "        \n",
    "    'mtb_min':      {'axis': 'mass',  'overflow':'over',  'bins': hist.Bin('pt', r'$min M_{T} (b, p_{T}^{miss}) \\ (GeV)$', 25, 0, 500)},\n",
    "    'lead_AK8_pt':  {'axis': 'pt',    'overflow':'over',  'bins': hist.Bin('pt', r'$p{T} (lead. AK8) \\ (GeV)$', 20, 0, 1000)},\n",
    "    'sublead_AK8_pt':  {'axis': 'pt',    'overflow':'over',  'bins': hist.Bin('pt', r'$p{T} (sublead. AK8) \\ (GeV)$', 20, 0, 1000)},\n",
    "    'lead_AK8_eta':  {'axis': 'eta',    'overflow':'over',  'bins': hist.Bin('eta', r'$\\eta (lead. AK8)$', 15, -5.5, 5.5)},\n",
    "    'sublead_AK8_eta':  {'axis': 'eta',    'overflow':'over',  'bins': hist.Bin('eta', r'$\\eta (sublead. AK8)$', 15, -5.5, 5.5)},\n",
    "    'mass':  {'axis': 'mass',    'overflow':'over',  'bins': hist.Bin('mass', r'softdrop mass', 20, 0, 200)},\n",
    "    }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from plots.helpers import *\n",
    "\n",
    "def saveFig( fig, ax, rax, path, name, scale='linear', shape=False, y_max=-1 ):\n",
    "    outdir = os.path.join(path,scale)\n",
    "    finalizePlotDir(outdir)\n",
    "    ax.set_yscale(scale)\n",
    "    ax.set_ylabel('Events')\n",
    "\n",
    "    if scale == 'linear':\n",
    "        if y_max<0: #or True:\n",
    "            pass\n",
    "        else:\n",
    "            ax.set_ylim(0, 1 if shape else 1.2*y_max)\n",
    "    else:\n",
    "        if y_max<0 and not shape:\n",
    "            pass\n",
    "        else:\n",
    "            ax.set_ylim(0.000005 if shape else 0.05, 3 if shape else 300*y_max)\n",
    "\n",
    "    handles, labels = ax.get_legend_handles_labels()\n",
    "    new_labels = []\n",
    "    for handle, label in zip(handles, labels):\n",
    "        #print (handle, label)\n",
    "        try:\n",
    "            new_labels.append(my_labels[label])\n",
    "            if not label=='pseudodata':\n",
    "                handle.set_color(colors[label])\n",
    "        except:\n",
    "            pass\n",
    "\n",
    "    if rax:\n",
    "        plt.subplots_adjust(hspace=0)\n",
    "        rax.set_ylabel('Obs./Pred.')\n",
    "        rax.set_ylim(0.0,2.0)\n",
    "\n",
    "    ax.legend(title='',ncol=2,handles=handles, labels=new_labels, frameon=False)\n",
    "\n",
    "    fig.text(0., 0.995, '$\\\\bf{CMS}$', fontsize=20,  horizontalalignment='left', verticalalignment='bottom', transform=ax.transAxes )\n",
    "    fig.text(0.15, 1., '$\\\\it{Simulation}$', fontsize=14, horizontalalignment='left', verticalalignment='bottom', transform=ax.transAxes )\n",
    "    fig.text(0.8, 1., '13 TeV', fontsize=14, horizontalalignment='left', verticalalignment='bottom', transform=ax.transAxes )\n",
    "\n",
    "    fig.savefig(os.path.join(outdir, \"{}.pdf\".format(name)))\n",
    "    fig.savefig(os.path.join(outdir, \"{}.png\".format(name)))\n",
    "    #ax.clear()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Histos I want to save\n",
    "histos = [[\"met_CR\", \"MET_ptCoarse\"],\n",
    "          [\"met_W_CR\", \"MET_ptCoarse\"],\n",
    "          [\"met_Higgs_CR\", \"MET_ptCoarse\"],\n",
    "          [\"met_Higgs_W_CR\", \"MET_ptCoarse\"],\n",
    "          \n",
    "          [\"ht_CR\", \"HT_Coarse\"],\n",
    "          [\"ht_W_CR\", \"HT_Coarse\"],\n",
    "          [\"ht_Higgs_CR\", \"HT_Coarse\"],\n",
    "          [\"ht_Higgs_W_CR\", \"HT_Coarse\"],\n",
    "          \n",
    "          [\"N_AK8_CR\", \"N_AK8\"],\n",
    "          [\"N_AK8_W_CR\", \"N_AK8\"],\n",
    "          [\"N_AK8_Higgs_CR\", \"N_AK8\"],\n",
    "          [\"N_AK8_Higgs_W_CR\", \"N_AK8\"],\n",
    "          \n",
    "          [\"lead_AK8_pt\", \"lead_AK8_pt\"],\n",
    "          [\"sublead_AK8_pt\", \"sublead_AK8_pt\"],\n",
    "          [\"lead_AK8_eta\", \"lead_AK8_eta\"],\n",
    "          [\"sublead_AK8_eta\", \"sublead_AK8_eta\"],\n",
    "          [\"lead_AK8_mass\", \"mass\"],\n",
    "          [\"sublead_AK8_mass\", \"mass\"],\n",
    "          [\"N_W_CR\", \"N_W\"],\n",
    "          [\"N_H_CR\", \"N_H\"],\n",
    "          [\"N_AK4_CR\", \"N_AK4\"],\n",
    "      \n",
    "          [\"W_pt_W_CR\", \"W_pt\"],\n",
    "          #[\"H_pt_W_CR\", \"H_pt\"],\n",
    "          [\"W_eta_W_CR\", \"W_eta\"],\n",
    "          #[\"H_eta_W_CR\", \"H_eta\"],\n",
    "          \n",
    "          #[\"W_pt_Higgs_CR\", \"W_pt\"],\n",
    "          [\"H_pt_Higgs_CR\", \"H_pt\"],\n",
    "          #[\"W_eta_Higgs_CR\", \"W_eta\"],\n",
    "          [\"H_eta_Higgs_CR\", \"H_eta\"],\n",
    "          \n",
    "          [\"W_pt_Higgs_W_CR\", \"W_pt\"],\n",
    "          [\"H_pt_Higgs_W_CR\", \"H_pt\"],\n",
    "          [\"W_eta_Higgs_W_CR\", \"W_eta\"],\n",
    "          [\"H_eta_Higgs_W_CR\", \"H_eta\"],\n",
    "         ]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Plotting aesthetics\n",
    "\n",
    "lineopts = {\n",
    "    'color': 'r',\n",
    "    'linewidth': '3'}\n",
    "\n",
    "data_err_opts = {\n",
    "    'linestyle': 'none',\n",
    "    'marker': '_',\n",
    "    'markersize': 10.,\n",
    "    'color': 'r',\n",
    "    'elinewidth': 1}\n",
    "\n",
    "data_err_opts_rat = {\n",
    "    'linestyle': 'none',\n",
    "    'marker': '.',\n",
    "    'markersize': 10.,\n",
    "    'color': 'k',\n",
    "    'elinewidth': 1}\n",
    "\n",
    "fillopts2 = {\n",
    "    'edgecolor': (0,0,0,0.3),\n",
    "    'facecolor': [('#989C94'),('#6A0136'),('#FF5714'),('#FFCA3A'),('#8AC926'),('#1982C4'),('#F76F8E')]  \n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Make some of the plots\n",
    "\n",
    "plotDir = '/home/users/ksalyer/public_html/dump/WH_had/'\n",
    "finalizePlotDir(plotDir)\n",
    "\n",
    "for plot in histos:\n",
    "\n",
    "    name = plot[0]\n",
    "    binName = plot[1]\n",
    "    print(name)\n",
    "    histogram = output[name]\n",
    "\n",
    "    axis = bins[binName]['axis']\n",
    "    histogram = histogram.rebin(axis, bins[binName]['bins'])\n",
    "\n",
    "    y_max = histogram.sum(\"dataset\").values(overflow='all')[()].max()\n",
    "    y_over = histogram.sum(\"dataset\").values(overflow='all')[()][-1]\n",
    "\n",
    "    import re\n",
    "    bkganddata = re.compile('(?!mC750_l1)')\n",
    "    bkgandsig = re.compile('(?!Data)')\n",
    "    \n",
    "    background = histogram[bkganddata][bkgandsig]\n",
    "    signal = histogram['mC750_l1']\n",
    "    data = histogram['Data']\n",
    "\n",
    "    #fig, ax = plt.subplots(1,1,figsize=(7,7))\n",
    "    fig, (ax, rax) = plt.subplots(nrows=2,ncols=1, figsize=(7,7),\n",
    "        gridspec_kw={\"height_ratios\": (3, 1)}, sharex=True)\n",
    "    \n",
    "    # get axes\n",
    "    #hist.plot1d(background, overlay=\"dataset\", ax=ax, stack=True, overflow=bins[binName]['overflow'], clear=False, fill_opts=fillopts2, error_opts=error_opts)#, order=['TTJets', 'ST', 'ttW', 'WW', 'ZNuNu', 'QCD', 'WJets']) #error_opts??\n",
    "    hist.plot1d(background, overlay=\"dataset\", ax=ax, stack=True, overflow=bins[binName]['overflow'], clear=False, fill_opts=fill_opts, error_opts=error_opts, order=['QCD', 'ZNuNu', 'WW', 'ttW', 'ST', 'WJets', 'TTJets']) #error_opts??\n",
    "    hist.plot1d(signal, overlay=\"dataset\", ax=ax, stack=False, overflow=bins[binName]['overflow'], line_opts=lineopts, clear=False)\n",
    "    hist.plot1d(data, overlay=\"dataset\", ax=ax, stack=False, overflow=bins[binName]['overflow'], error_opts=data_err_opts_rat, clear=False)\n",
    "\n",
    "    hist.plotratio(num=data.sum('dataset'), denom=background.sum('dataset'), ax=rax,\n",
    "                   error_opts = data_err_opts_rat, denom_fill_opts={}, guide_opts={}, \n",
    "                   unc='num', overflow = 'over')\n",
    "\n",
    "    for l in ['log', 'linear']:\n",
    "        saveFig(fig, ax, rax, plotDir, name, scale=l, shape=False, y_max=y_max)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'plotDir = \\'/home/users/ksalyer/public_html/dump/WH_had/\\'\\nfinalizePlotDir(plotDir)\\n\\nif True:\\n    name = \\'MET_pt\\'\\n\\n    histogram = output[\\'met_CR\\']\\n\\n    axis = bins[name][\\'axis\\']\\n    histogram = histogram.rebin(axis, bins[name][\\'bins\\'])\\n\\n    y_max = histogram.sum(\"dataset\").values(overflow=\\'all\\')[()].max()\\n    y_over = histogram.sum(\"dataset\").values(overflow=\\'all\\')[()][-1]\\n\\n    import re\\n    bkgonly = re.compile(\\'(?!mC750_l1)\\')\\n    notdata = re.compile(\\'(?!pseudodata)\\')\\n    notsignal = re.compile(\\'(?!mC750_l1)\\')\\n\\n    fig, ax = plt.subplots(1,1,figsize=(7,7))\\n    \\n    # get axes\\n    hist.plot1d(histogram[bkgonly], overlay=\"dataset\", ax=ax, stack=True, overflow=bins[name][\\'overflow\\'], clear=False, line_opts=None, fill_opts=fill_opts, error_opts=error_opts)#, order=[\\'QCD\\', \\'ZNuNu\\', \\'LL\\']) #error_opts??\\n    hist.plot1d(histogram[\\'mC750_l1\\'], overlay=\"dataset\", ax=ax, overflow=bins[name][\\'overflow\\'], line_opts={\\'linewidth\\':3}, clear=False)\\n\\n    for l in [\\'log\\']:\\n        saveFig(fig, ax, None, plotDir, name, scale=l, shape=False, y_max=y_max)\\n'"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Make some of the plots\n",
    "\n",
    "'''plotDir = '/home/users/ksalyer/public_html/dump/WH_had/'\n",
    "finalizePlotDir(plotDir)\n",
    "\n",
    "if True:\n",
    "    name = 'MET_pt'\n",
    "\n",
    "    histogram = output['met_CR']\n",
    "\n",
    "    axis = bins[name]['axis']\n",
    "    histogram = histogram.rebin(axis, bins[name]['bins'])\n",
    "\n",
    "    y_max = histogram.sum(\"dataset\").values(overflow='all')[()].max()\n",
    "    y_over = histogram.sum(\"dataset\").values(overflow='all')[()][-1]\n",
    "\n",
    "    import re\n",
    "    bkgonly = re.compile('(?!mC750_l1)')\n",
    "    notdata = re.compile('(?!pseudodata)')\n",
    "    notsignal = re.compile('(?!mC750_l1)')\n",
    "\n",
    "    fig, ax = plt.subplots(1,1,figsize=(7,7))\n",
    "    \n",
    "    # get axes\n",
    "    hist.plot1d(histogram[bkgonly], overlay=\"dataset\", ax=ax, stack=True, overflow=bins[name]['overflow'], clear=False, line_opts=None, fill_opts=fill_opts, error_opts=error_opts)#, order=['QCD', 'ZNuNu', 'LL']) #error_opts??\n",
    "    hist.plot1d(histogram['mC750_l1'], overlay=\"dataset\", ax=ax, overflow=bins[name]['overflow'], line_opts={'linewidth':3}, clear=False)\n",
    "\n",
    "    for l in ['log']:\n",
    "        saveFig(fig, ax, None, plotDir, name, scale=l, shape=False, y_max=y_max)\n",
    "'''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "#histogram = output['dphiDiFatJet']\n",
    "#ax = hist.plot1d(histogram,overlay=\"dataset\", stack=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "#histogram = output['WH_deltaR']\n",
    "#ax = hist.plot1d(histogram,overlay=\"dataset\", stack=False)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "coffeaEnv",
   "language": "python",
   "name": "coffeaenv"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
